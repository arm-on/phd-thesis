{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23568,
     "status": "ok",
     "timestamp": 1665143809039,
     "user": {
      "displayName": "Arman Malekzadeh",
      "userId": "04598358352447415958"
     },
     "user_tz": -210
    },
    "id": "o8SHJpX5WLTk",
    "outputId": "0c751eb1-76a3-424c-fab0-f9dec029009a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.5.2)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (5.0.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.10.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.8.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (2022.4)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: persiantools in /usr/local/lib/python3.7/dist-packages (3.0.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: adversarial-robustness-toolbox in /usr/local/lib/python3.7/dist-packages (1.12.1)\n",
      "Requirement already satisfied: scikit-learn<1.1.0,>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (1.0.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (1.21.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (57.4.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (1.15.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (1.7.3)\n",
      "Requirement already satisfied: numba>=0.53.1 in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (0.56.2)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.53.1->adversarial-robustness-toolbox) (0.39.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.53.1->adversarial-robustness-toolbox) (5.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.1.0,>=0.22.2->adversarial-robustness-toolbox) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.1.0,>=0.22.2->adversarial-robustness-toolbox) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.53.1->adversarial-robustness-toolbox) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.53.1->adversarial-robustness-toolbox) (4.1.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets\n",
    "# !pip install torchmetrics\n",
    "# !pip install pytz\n",
    "# !pip install persiantools\n",
    "# !pip install adversarial-robustness-toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4pFNCaeo4T1"
   },
   "source": [
    "# Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6151,
     "status": "ok",
     "timestamp": 1665143815166,
     "user": {
      "displayName": "Arman Malekzadeh",
      "userId": "04598358352447415958"
     },
     "user_tz": -210
    },
    "id": "8D8BYQHpoWqf",
    "outputId": "8fdfecd9-9ca2-4123-b792-a29dd8b991a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1665143815166,
     "user": {
      "displayName": "Arman Malekzadeh",
      "userId": "04598358352447415958"
     },
     "user_tz": -210
    },
    "id": "8Q0i0uh2ouQN",
    "outputId": "c47dadb8-3c5f-40b1-8bc1-afe254d90d98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive\n"
     ]
    }
   ],
   "source": [
    "# %cd /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AnpOkamrk6V"
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0G5mWy4A4g0"
   },
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['root_path'] = '/home/user01/'\n",
    "config['series_ID'] = 101\n",
    "config['series_desc'] = '''\n",
    "sample description\n",
    "'''\n",
    "config['log_path'] = config['root_path']+'experiments/MNIST/LR/'\n",
    "config['log'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAOW8s9cZXbj"
   },
   "outputs": [],
   "source": [
    "config['poisoning_rate'] = 0.8\n",
    "# config['num_clean_examples'] = 200\n",
    "config['learning_rate'] = 0.01\n",
    "config['batch_size'] = 32\n",
    "config['num_epochs'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OK99HTxqdza0"
   },
   "outputs": [],
   "source": [
    "config['log']['model'] = 'LR'\n",
    "config['log']['dataset'] = 'MNIST (0-1)'\n",
    "config['log']['task'] = 'binary classification'\n",
    "config['log']['pytorch_seed'] = 50\n",
    "config['log']['numpy_seed'] = 50\n",
    "config['log']['attack'] = 'label-flip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nmw4UOK5ZjTA"
   },
   "outputs": [],
   "source": [
    "config['log_path'] += (str(config['series_ID']) + '-' + config['log']['attack'] + '-' + config['log']['dataset'] + '-' + str(int(config['poisoning_rate']*100)) + '.json').lower().replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eCH1zUVZtR7"
   },
   "outputs": [],
   "source": [
    "config['log_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LnFCylB4lSh0"
   },
   "outputs": [],
   "source": [
    "### WE NEED THIS TO IMPORT THE NECESSARY LIBRARIES ###\n",
    "import sys\n",
    "sys.path.append(config['root_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnyIFAVYTdr9"
   },
   "outputs": [],
   "source": [
    "import datascience, poisoning, report\n",
    "from datascience.data import CIFAR10, MNIST, IMDB, BOSTON\n",
    "from datascience.general import train_dev_test_split, join_np_arrays, describe_dataset\n",
    "from poisoning.process import attacker, defender, label_flip_attacker\n",
    "from poisoning.eval import attack_success_rate, benign_accuracy, test_accuracy\n",
    "from report.log import JSONLogger, TextLogger, tehran_datetime\n",
    "from temporary.functions import _reload\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchmetrics import HingeLoss\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd.functional import hessian, jacobian\n",
    "from torch.autograd import grad\n",
    "from torch.nn.utils import _stateless\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from persiantools.jdatetime import JalaliDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_P8MfaXKWWBX"
   },
   "outputs": [],
   "source": [
    "# _reload(poisoning.process)\n",
    "# _reload(poisoning.eval)\n",
    "# _reload(datascience.data)\n",
    "# _reload(datascience.general)\n",
    "# _reload(report.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1665143824805,
     "user": {
      "displayName": "Arman Malekzadeh",
      "userId": "04598358352447415958"
     },
     "user_tz": -210
    },
    "id": "pIDVaGyYi7dU",
    "outputId": "eddb7e66-1064-4739-8be5-b879f76e146f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4c055841d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(config['log']['pytorch_seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Xd4fx8XpN3Z"
   },
   "source": [
    "# Loading a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tH11gD3oD9T9"
   },
   "outputs": [],
   "source": [
    "dataset = MNIST()\n",
    "dataset.select_labels([0,1], phase='train')\n",
    "dataset.select_labels([0,1], phase='test')\n",
    "dataset.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnX5EJKMKR91"
   },
   "outputs": [],
   "source": [
    "# dataset.separate_examples(config['num_clean_examples'], desired_seed = config['log']['numpy_seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpmO2y9slv7f"
   },
   "outputs": [],
   "source": [
    "# dataset.change_labels({0:-1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jw03ns8FLHf-"
   },
   "source": [
    "# Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-emHqmgILJND"
   },
   "outputs": [],
   "source": [
    "att = label_flip_attacker(dataset.y_train, {0:1, 1:0}, attack_prob=config['poisoning_rate'], desired_seed=config['log']['numpy_seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhvjuGCyOl61"
   },
   "outputs": [],
   "source": [
    "dataset.y_train = att.attack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGpCEzyOOIEl"
   },
   "outputs": [],
   "source": [
    "# y_train_all = np.concatenate((dataset.y_train_att, dataset.y_train_clean), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iajxP5YqOuEo"
   },
   "outputs": [],
   "source": [
    "# x_train_all = np.concatenate((dataset.x_train, dataset.x_train_clean), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erII30KGS5Jd"
   },
   "outputs": [],
   "source": [
    "config['data-train'] = describe_dataset(dataset.x_train, dataset.y_train, 'training dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikGmGA85VN-r"
   },
   "outputs": [],
   "source": [
    "config['data-test'] = describe_dataset(dataset.x_test, dataset.y_test, 'testing dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdBVWI7eXmm-"
   },
   "source": [
    "# Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bzau-0FrWZOS"
   },
   "outputs": [],
   "source": [
    "class MyVectorDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = np.array(labels).reshape(-1, 1)\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.Tensor(self.features[idx]).to(device), torch.Tensor(self.labels[idx]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYDMRGv2XvCz"
   },
   "outputs": [],
   "source": [
    "train_dataset = MyVectorDataset(dataset.x_train, dataset.y_train)\n",
    "test_dataset = MyVectorDataset(dataset.x_test, dataset.y_test)\n",
    "# clean_dataset = MyVectorDataset(dataset.x_train_clean, dataset.y_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGJs6c6uXzlr"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "# clean_dataloader = DataLoader(clean_dataset, batch_size=config['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrYWXRXRYtE3"
   },
   "source": [
    "# Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnEaduMNYt2n"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        outputs = torch.sigmoid(self.linear(x))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTt67qS9Y2bk"
   },
   "outputs": [],
   "source": [
    "def output_to_label(out):\n",
    "    if out >= 0.5:\n",
    "      return 1\n",
    "    else:\n",
    "      return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ye_BrNp8tPaJ"
   },
   "outputs": [],
   "source": [
    "def make_prediction(model, x_arr):\n",
    "  outs = list(model(torch.Tensor(x_arr)).squeeze().detach().numpy())\n",
    "  labels = [output_to_label(out) for out in outs]\n",
    "  return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAgyHS15Yvq_"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(784,1)\n",
    "model = model.to(device)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rqqo59u4ZgDN"
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, epoch_num):\n",
    "    num_points = len(dataloader.dataset)\n",
    "    for batch, (features, labels) in enumerate(dataloader):        \n",
    "        # Compute prediction and loss\n",
    "        pred = model(features)\n",
    "        loss = loss_fn(pred, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # sets gradients of all model parameters to zero\n",
    "        loss.backward() # calculate the gradients again\n",
    "        optimizer.step() # w = w - learning_rate * grad(loss)_with_respect_to_w\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(features)\n",
    "            print(f\"\\r Epoch {epoch_num} - loss: {loss:>7f}  [{current:>5d}/{num_points:>5d}]\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHBqO-s_ZiKL"
   },
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn, epoch_num, name):\n",
    "    num_points = len(dataloader.dataset)\n",
    "    sum_test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (features, labels) in enumerate(dataloader):\n",
    "            pred = model(features)\n",
    "            sum_test_loss += loss_fn(pred, labels).item() # add the current loss to the sum of the losses\n",
    "            # convert the outputs of the model on the current batch to a numpy array\n",
    "            pred_lst = list(pred.cpu().numpy().squeeze())\n",
    "            pred_lst = [output_to_label(item) for item in pred_lst]\n",
    "            # convert the original labels corresponding to the current batch to a numpy array\n",
    "            output_lst = list(labels.cpu().numpy().squeeze()) \n",
    "            # determine the points for which the model is correctly predicting the label (add a 1 for each)\n",
    "            match_lst = [1 if p==o else 0 for (p, o) in zip(pred_lst, output_lst)] \n",
    "            # count how many points are labeled correctly in this batch and add the number to the overall count of the correct labeled points\n",
    "            correct += sum(match_lst) \n",
    "            \n",
    "    sum_test_loss /= num_points\n",
    "    correct /= num_points\n",
    "    config['log']['accuracy_'+name] = (100*correct)\n",
    "    config['log']['loss_'+name] = sum_test_loss\n",
    "    print(f\"\\r Epoch {epoch_num} - {name} Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {sum_test_loss:>8f}\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1104,
     "status": "ok",
     "timestamp": 1665143825873,
     "user": {
      "displayName": "Arman Malekzadeh",
      "userId": "04598358352447415958"
     },
     "user_tz": -210
    },
    "id": "RmFavtrVZlCz",
    "outputId": "9ec5ae00-9b5f-4288-9f03-5cf52533186d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 5 - loss: 0.000000  [ 9600/12665] "
     ]
    }
   ],
   "source": [
    "for epoch_num in range(1, config['num_epochs']+1):\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, epoch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1665143825873,
     "user": {
      "displayName": "Arman Malekzadeh",
      "userId": "04598358352447415958"
     },
     "user_tz": -210
    },
    "id": "mJAb_4ey5WXp",
    "outputId": "426104a0-2d31-4deb-b7c4-a0116a951484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " Epoch 5 - Train Error: Accuracy: 99.8%, Avg loss: 0.005182 "
     ]
    }
   ],
   "source": [
    "test_loop(train_dataloader, model, loss_fn, config['num_epochs'], 'Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1665143825874,
     "user": {
      "displayName": "Arman Malekzadeh",
      "userId": "04598358352447415958"
     },
     "user_tz": -210
    },
    "id": "YzY6AKTBZsHZ",
    "outputId": "f9aca5ca-c070-493c-f641-86b1337bf9f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " Epoch 5 - Test Error: Accuracy: 0.0%, Avg loss: 3.164557 "
     ]
    }
   ],
   "source": [
    "test_loop(test_dataloader, model, loss_fn, config['num_epochs'], 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYghCJpZK1HZ"
   },
   "outputs": [],
   "source": [
    "config['datetime'] = tehran_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1844,
     "status": "ok",
     "timestamp": 1665143827712,
     "user": {
      "displayName": "Arman Malekzadeh",
      "userId": "04598358352447415958"
     },
     "user_tz": -210
    },
    "id": "HKd5CFupCcW7",
    "outputId": "6bfae5f2-9df3-43a2-a57c-cce35c0f1a3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log': {'model': 'Logistic Regression',\n",
       "  'dataset': 'MNIST',\n",
       "  'task': 'binary classification (0-1) - LabelFlip Attack',\n",
       "  'pytorch_seed': 50,\n",
       "  'numpy_seed': 50,\n",
       "  'accuracy_Train': 99.83418870904066,\n",
       "  'loss_Train': 0.005181602842479273,\n",
       "  'accuracy_Test': 0.04728132387706856,\n",
       "  'loss_Test': 3.164556888372904},\n",
       " 'log_path': '/content/drive/MyDrive/experiments/MNIST/LR/labelflip-01-80.json',\n",
       " 'poisoning_rate': 0.8,\n",
       " 'learning_rate': 0.01,\n",
       " 'batch_size': 32,\n",
       " 'num_epochs': 5,\n",
       " 'data-train': {'name': 'training dataset',\n",
       "  'num_samples': 12665,\n",
       "  'num_features': (784,),\n",
       "  'class_count': {0: 6742, 1: 5923}},\n",
       " 'data-test': {'name': 'testing dataset',\n",
       "  'num_samples': 2115,\n",
       "  'num_features': (784,),\n",
       "  'class_count': {0: 980, 1: 1135}},\n",
       " 'datetime': '1401-07-15 15:27'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAIGh8n7__Ge"
   },
   "outputs": [],
   "source": [
    "logger = JSONLogger(config['log_path'], config)\n",
    "logger.log()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMRta4dK5mPRO5iQBIn1uQs",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
