{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62b5b906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 19:41:40.962390: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-14 19:41:41.166469: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-05-14 19:41:41.853688: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: usr/local/cuda-11.8/lib64\n",
      "2023-05-14 19:41:41.853865: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: usr/local/cuda-11.8/lib64\n",
      "2023-05-14 19:41:41.853877: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from keras.utils import to_categorical\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac9911c",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9943fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size':128,\n",
    "    'learning_rate':1e-5,\n",
    "    'weight_decay':5e-4,\n",
    "    'num_epochs':20\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3644bfa",
   "metadata": {},
   "source": [
    "Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddaeb5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (0.5071, 0.4867, 0.4408)\n",
    "std = (0.2675, 0.2565, 0.2761)\n",
    "\n",
    "train_transform = [transforms.Resize((224, 224)),\n",
    "                   transforms.RandomHorizontalFlip(p=0.5),\n",
    "                   transforms.RandomVerticalFlip(p=0.5),\n",
    "                   transforms.ToTensor(),\n",
    "                   transforms.Normalize(mean, std),\n",
    "                  ]\n",
    "train_transform = transforms.Compose(train_transform)\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean, std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69bbe064",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_trainset = datasets.CIFAR100(root='/home/user01/cifar100/train/', train=True,\n",
    "                                    download=False, transform=train_transform)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    cifar100_trainset, batch_size=config['batch_size'], shuffle=True, num_workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb1c0e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_testset = datasets.CIFAR100(root='/home/user01/cifar100/test/', train=False,\n",
    "                                   download=False, transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    cifar100_testset, batch_size=config['batch_size'], shuffle=False, num_workers=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f901ee",
   "metadata": {},
   "source": [
    "Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f55f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_graph_node_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3590efaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = models.resnet34(weights='ResNet34_Weights.IMAGENET1K_V1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81bc5cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in new_model.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67d1333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.fc = torch.nn.Linear(in_features=512, out_features=100, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "978aecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = np.random.rand(2,3,32,32)\n",
    "# new_model(torch.Tensor(inp[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a599dab",
   "metadata": {},
   "source": [
    "# Finetuning with CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30cef50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_out(label):\n",
    "    out = np.zeros(100)\n",
    "    out[label] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d84d661a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af46e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = new_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50eb53ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39cdb9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "layer1.0.conv1.weight\n",
      "layer1.0.bn1.weight\n",
      "layer1.0.bn1.bias\n",
      "layer1.0.conv2.weight\n",
      "layer1.0.bn2.weight\n",
      "layer1.0.bn2.bias\n",
      "layer1.1.conv1.weight\n",
      "layer1.1.bn1.weight\n",
      "layer1.1.bn1.bias\n",
      "layer1.1.conv2.weight\n",
      "layer1.1.bn2.weight\n",
      "layer1.1.bn2.bias\n",
      "layer1.2.conv1.weight\n",
      "layer1.2.bn1.weight\n",
      "layer1.2.bn1.bias\n",
      "layer1.2.conv2.weight\n",
      "layer1.2.bn2.weight\n",
      "layer1.2.bn2.bias\n",
      "layer2.0.conv1.weight\n",
      "layer2.0.bn1.weight\n",
      "layer2.0.bn1.bias\n",
      "layer2.0.conv2.weight\n",
      "layer2.0.bn2.weight\n",
      "layer2.0.bn2.bias\n",
      "layer2.0.downsample.0.weight\n",
      "layer2.0.downsample.1.weight\n",
      "layer2.0.downsample.1.bias\n",
      "layer2.1.conv1.weight\n",
      "layer2.1.bn1.weight\n",
      "layer2.1.bn1.bias\n",
      "layer2.1.conv2.weight\n",
      "layer2.1.bn2.weight\n",
      "layer2.1.bn2.bias\n",
      "layer2.2.conv1.weight\n",
      "layer2.2.bn1.weight\n",
      "layer2.2.bn1.bias\n",
      "layer2.2.conv2.weight\n",
      "layer2.2.bn2.weight\n",
      "layer2.2.bn2.bias\n",
      "layer2.3.conv1.weight\n",
      "layer2.3.bn1.weight\n",
      "layer2.3.bn1.bias\n",
      "layer2.3.conv2.weight\n",
      "layer2.3.bn2.weight\n",
      "layer2.3.bn2.bias\n",
      "layer3.0.conv1.weight\n",
      "layer3.0.bn1.weight\n",
      "layer3.0.bn1.bias\n",
      "layer3.0.conv2.weight\n",
      "layer3.0.bn2.weight\n",
      "layer3.0.bn2.bias\n",
      "layer3.0.downsample.0.weight\n",
      "layer3.0.downsample.1.weight\n",
      "layer3.0.downsample.1.bias\n",
      "layer3.1.conv1.weight\n",
      "layer3.1.bn1.weight\n",
      "layer3.1.bn1.bias\n",
      "layer3.1.conv2.weight\n",
      "layer3.1.bn2.weight\n",
      "layer3.1.bn2.bias\n",
      "layer3.2.conv1.weight\n",
      "layer3.2.bn1.weight\n",
      "layer3.2.bn1.bias\n",
      "layer3.2.conv2.weight\n",
      "layer3.2.bn2.weight\n",
      "layer3.2.bn2.bias\n",
      "layer3.3.conv1.weight\n",
      "layer3.3.bn1.weight\n",
      "layer3.3.bn1.bias\n",
      "layer3.3.conv2.weight\n",
      "layer3.3.bn2.weight\n",
      "layer3.3.bn2.bias\n",
      "layer3.4.conv1.weight\n",
      "layer3.4.bn1.weight\n",
      "layer3.4.bn1.bias\n",
      "layer3.4.conv2.weight\n",
      "layer3.4.bn2.weight\n",
      "layer3.4.bn2.bias\n",
      "layer3.5.conv1.weight\n",
      "layer3.5.bn1.weight\n",
      "layer3.5.bn1.bias\n",
      "layer3.5.conv2.weight\n",
      "layer3.5.bn2.weight\n",
      "layer3.5.bn2.bias\n",
      "layer4.0.conv1.weight\n",
      "layer4.0.bn1.weight\n",
      "layer4.0.bn1.bias\n",
      "layer4.0.conv2.weight\n",
      "layer4.0.bn2.weight\n",
      "layer4.0.bn2.bias\n",
      "layer4.0.downsample.0.weight\n",
      "layer4.0.downsample.1.weight\n",
      "layer4.0.downsample.1.bias\n",
      "layer4.1.conv1.weight\n",
      "layer4.1.bn1.weight\n",
      "layer4.1.bn1.bias\n",
      "layer4.1.conv2.weight\n",
      "layer4.1.bn2.weight\n",
      "layer4.1.bn2.bias\n",
      "layer4.2.conv1.weight\n",
      "layer4.2.bn1.weight\n",
      "layer4.2.bn1.bias\n",
      "layer4.2.conv2.weight\n",
      "layer4.2.bn2.weight\n",
      "layer4.2.bn2.bias\n",
      "fc.weight\n",
      "fc.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in new_model.named_parameters():\n",
    "    print(f'{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fa98052",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_1x = [param for name, param in new_model.named_parameters() if 'fc' not in str(name)]\n",
    "optimizer = torch.optim.Adam([{'params':params_1x}, {'params': new_model.fc.parameters(), 'lr': config['learning_rate']*10}],\n",
    "                             lr=config['learning_rate'], weight_decay=config['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e004d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_label(out):\n",
    "    return out.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c53ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch, (features, labels) in enumerate(trainloader):\n",
    "#     if batch == 0:\n",
    "#         out = new_model(features.to(device))\n",
    "#         print(out.shape)\n",
    "#         print(features.shape)\n",
    "#         print(labels.shape)\n",
    "#         print(loss_fn(out, labels.to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c3385f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, epoch_num):\n",
    "    num_points = len(dataloader.dataset)\n",
    "    for batch, (features, labels) in enumerate(dataloader):        \n",
    "        # Compute prediction and loss\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        pred = model(features)\n",
    "        loss = loss_fn(pred, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # sets gradients of all model parameters to zero\n",
    "        loss.backward() # calculate the gradients again\n",
    "        optimizer.step() # w = w - learning_rate * grad(loss)_with_respect_to_w\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(features)\n",
    "            print(f\"Epoch {epoch_num} - loss: {loss:>7f}  [{current:>5d}/{num_points:>5d}]\\n\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, epoch_num, name):\n",
    "    num_points = len(dataloader.dataset)\n",
    "    sum_test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (features, labels) in enumerate(dataloader):\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            pred = model(features)\n",
    "            if batch == 0:\n",
    "                global tmp_features\n",
    "                global tmp_labels\n",
    "                global tmp_pred\n",
    "                tmp_features = features\n",
    "                tmp_labels = labels\n",
    "                tmp_pred = pred\n",
    "            sum_test_loss += loss_fn(pred, labels).item() # add the current loss to the sum of the losses\n",
    "            # convert the outputs of the model on the current batch to a numpy array\n",
    "            pred_lst = list(pred.argmax(axis=1).cpu().numpy())\n",
    "            # convert the original labels corresponding to the current batch to a numpy array\n",
    "            true_lst = labels\n",
    "            # determine the points for which the model is correctly predicting the label (add a 1 for each)\n",
    "            match_lst = [1 if p==t else 0 for (p, t) in zip(pred_lst, true_lst)] \n",
    "            # count how many points are labeled correctly in this batch and add the number to the overall count of the correct labeled points\n",
    "            correct += sum(match_lst) \n",
    "            \n",
    "    sum_test_loss /= num_points\n",
    "    correct /= num_points\n",
    "    print(f\"Epoch {epoch_num} - {name} Accuracy: {correct*100}%, Avg loss: {sum_test_loss}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d539702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - loss: 4.762279  [    0/50000]\n",
      "\n",
      "Epoch 1 - loss: 4.057532  [12800/50000]\n",
      "\n",
      "Epoch 1 - loss: 3.181620  [25600/50000]\n",
      "\n",
      "Epoch 1 - loss: 2.471166  [38400/50000]\n",
      "\n",
      "Epoch 1 - Test Accuracy: 53.010000000000005%, Avg loss: 0.016136525762081147\n",
      "\n",
      "Epoch 2 - loss: 2.079953  [    0/50000]\n",
      "\n",
      "Epoch 2 - loss: 1.693331  [12800/50000]\n",
      "\n",
      "Epoch 2 - loss: 1.651588  [25600/50000]\n",
      "\n",
      "Epoch 2 - loss: 1.542446  [38400/50000]\n",
      "\n",
      "Epoch 2 - Test Accuracy: 65.60000000000001%, Avg loss: 0.010418665897846222\n",
      "\n",
      "Epoch 3 - loss: 1.528598  [    0/50000]\n",
      "\n",
      "Epoch 3 - Test Accuracy: 70.27%, Avg loss: 0.008427358275651931\n",
      "\n",
      "Epoch 4 - loss: 1.282648  [    0/50000]\n",
      "\n",
      "Epoch 4 - loss: 1.029013  [12800/50000]\n",
      "\n",
      "Epoch 4 - loss: 0.970660  [25600/50000]\n",
      "\n",
      "Epoch 4 - loss: 1.121636  [38400/50000]\n",
      "\n",
      "Epoch 4 - Test Accuracy: 72.97%, Avg loss: 0.007434784436225891\n",
      "\n",
      "Epoch 5 - loss: 0.964223  [    0/50000]\n",
      "\n",
      "Epoch 5 - loss: 1.096099  [12800/50000]\n",
      "\n",
      "Epoch 5 - loss: 0.914262  [25600/50000]\n",
      "\n",
      "Epoch 5 - loss: 0.869689  [38400/50000]\n",
      "\n",
      "Epoch 5 - Test Accuracy: 74.7%, Avg loss: 0.006798473846912384\n",
      "\n",
      "Epoch 6 - loss: 0.881289  [    0/50000]\n",
      "\n",
      "Epoch 6 - Test Accuracy: 76.14999999999999%, Avg loss: 0.006395740485191345\n",
      "\n",
      "Epoch 7 - loss: 0.657625  [    0/50000]\n",
      "\n",
      "Epoch 7 - loss: 0.600761  [12800/50000]\n",
      "\n",
      "Epoch 7 - loss: 0.717355  [25600/50000]\n",
      "\n",
      "Epoch 7 - loss: 0.646119  [38400/50000]\n",
      "\n",
      "Epoch 7 - Test Accuracy: 77.11%, Avg loss: 0.006103485760092735\n",
      "\n",
      "Epoch 8 - loss: 0.460387  [    0/50000]\n",
      "\n",
      "Epoch 8 - loss: 0.594450  [12800/50000]\n",
      "\n",
      "Epoch 8 - loss: 0.656555  [25600/50000]\n",
      "\n",
      "Epoch 8 - loss: 0.559681  [38400/50000]\n",
      "\n",
      "Epoch 8 - Test Accuracy: 77.8%, Avg loss: 0.0058705293387174605\n",
      "\n",
      "Epoch 9 - loss: 0.671326  [    0/50000]\n",
      "\n",
      "Epoch 9 - loss: 0.517177  [12800/50000]\n",
      "\n",
      "Epoch 9 - loss: 0.449526  [25600/50000]\n",
      "\n",
      "Epoch 9 - loss: 0.560640  [38400/50000]\n",
      "\n",
      "Epoch 9 - Test Accuracy: 78.21000000000001%, Avg loss: 0.0057438315838575366\n",
      "\n",
      "Epoch 10 - loss: 0.605837  [    0/50000]\n",
      "\n",
      "Epoch 10 - loss: 0.518072  [12800/50000]\n",
      "\n",
      "Epoch 10 - loss: 0.463758  [25600/50000]\n",
      "\n",
      "Epoch 10 - loss: 0.632578  [38400/50000]\n",
      "\n",
      "Epoch 10 - Test Accuracy: 78.77%, Avg loss: 0.0056499784469604495\n",
      "\n",
      "Epoch 11 - loss: 0.526120  [    0/50000]\n",
      "\n",
      "Epoch 11 - loss: 0.370258  [12800/50000]\n",
      "\n",
      "Epoch 11 - loss: 0.411373  [25600/50000]\n",
      "\n",
      "Epoch 11 - loss: 0.392276  [38400/50000]\n",
      "\n",
      "Epoch 11 - Test Accuracy: 78.91%, Avg loss: 0.005575746649503708\n",
      "\n",
      "Epoch 12 - loss: 0.392773  [    0/50000]\n",
      "\n",
      "Epoch 12 - loss: 0.447024  [12800/50000]\n",
      "\n",
      "Epoch 12 - loss: 0.311627  [25600/50000]\n",
      "\n",
      "Epoch 12 - loss: 0.471244  [38400/50000]\n",
      "\n",
      "Epoch 12 - Test Accuracy: 79.31%, Avg loss: 0.005468407988548279\n",
      "\n",
      "Epoch 13 - loss: 0.373776  [    0/50000]\n",
      "\n",
      "Epoch 13 - loss: 0.331872  [12800/50000]\n",
      "\n",
      "Epoch 13 - loss: 0.390758  [25600/50000]\n",
      "\n",
      "Epoch 13 - loss: 0.394946  [38400/50000]\n",
      "\n",
      "Epoch 13 - Test Accuracy: 79.45%, Avg loss: 0.00544803082048893\n",
      "\n",
      "Epoch 14 - loss: 0.283426  [    0/50000]\n",
      "\n",
      "Epoch 14 - loss: 0.358744  [12800/50000]\n",
      "\n",
      "Epoch 14 - loss: 0.280180  [25600/50000]\n",
      "\n",
      "Epoch 14 - loss: 0.309835  [38400/50000]\n",
      "\n",
      "Epoch 14 - Test Accuracy: 79.74%, Avg loss: 0.00545942502617836\n",
      "\n",
      "Epoch 15 - loss: 0.305190  [    0/50000]\n",
      "\n",
      "Epoch 15 - loss: 0.280397  [12800/50000]\n",
      "\n",
      "Epoch 15 - loss: 0.261984  [25600/50000]\n",
      "\n",
      "Epoch 15 - loss: 0.185776  [38400/50000]\n",
      "\n",
      "Epoch 15 - Test Accuracy: 79.84%, Avg loss: 0.005442396593093872\n",
      "\n",
      "Epoch 16 - loss: 0.337430  [    0/50000]\n",
      "\n",
      "Epoch 16 - loss: 0.250086  [12800/50000]\n",
      "\n",
      "Epoch 16 - loss: 0.340151  [25600/50000]\n",
      "\n",
      "Epoch 16 - loss: 0.249771  [38400/50000]\n",
      "\n",
      "Epoch 16 - Test Accuracy: 80.0%, Avg loss: 0.005482733148336411\n",
      "\n",
      "Epoch 17 - loss: 0.244551  [    0/50000]\n",
      "\n",
      "Epoch 17 - loss: 0.216742  [12800/50000]\n",
      "\n",
      "Epoch 17 - loss: 0.245912  [25600/50000]\n",
      "\n",
      "Epoch 17 - loss: 0.215777  [38400/50000]\n",
      "\n",
      "Epoch 17 - Test Accuracy: 80.16%, Avg loss: 0.005451152059435844\n",
      "\n",
      "Epoch 18 - loss: 0.268875  [    0/50000]\n",
      "\n",
      "Epoch 18 - loss: 0.184750  [12800/50000]\n",
      "\n",
      "Epoch 18 - loss: 0.197094  [25600/50000]\n",
      "\n",
      "Epoch 18 - loss: 0.236638  [38400/50000]\n",
      "\n",
      "Epoch 18 - Test Accuracy: 80.17999999999999%, Avg loss: 0.005510968202352523\n",
      "\n",
      "Epoch 19 - loss: 0.177567  [    0/50000]\n",
      "\n",
      "Epoch 19 - loss: 0.193377  [12800/50000]\n",
      "\n",
      "Epoch 19 - loss: 0.224570  [25600/50000]\n",
      "\n",
      "Epoch 19 - loss: 0.215177  [38400/50000]\n",
      "\n",
      "Epoch 19 - Test Accuracy: 80.32000000000001%, Avg loss: 0.005542751923203468\n",
      "\n",
      "Epoch 20 - loss: 0.262770  [    0/50000]\n",
      "\n",
      "Epoch 20 - loss: 0.155724  [12800/50000]\n",
      "\n",
      "Epoch 20 - loss: 0.131094  [25600/50000]\n",
      "\n",
      "Epoch 20 - loss: 0.205389  [38400/50000]\n",
      "\n",
      "Epoch 20 - Test Accuracy: 80.32000000000001%, Avg loss: 0.005602649679780006\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch_num in range(1, config['num_epochs']+1):\n",
    "    train_loop(trainloader, new_model, loss_fn, optimizer, epoch_num)\n",
    "    test_loop(testloader, new_model, loss_fn, epoch_num, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e860e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_model, 'cifar100.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf002987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loop(testloader, new_model, loss_fn, epoch_num, 'Test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
