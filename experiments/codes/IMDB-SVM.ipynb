{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb9a5e2a",
   "metadata": {},
   "source": [
    "# Install Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b96fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install torchmetrics\n",
    "# !pip install pytz\n",
    "# !pip install persiantools\n",
    "# !pip install adversarial-robustness-toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62e286",
   "metadata": {},
   "source": [
    "# Google Drive Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8244ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "222ed25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a88fc6",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a51f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['root_path'] = '/home/user01/' # this is where the experiments folder exists\n",
    "config['series_ID'] = 103\n",
    "config['series_desc'] = '''\n",
    "targeted backdoor attack labels changed - LR influence attack manually implemented (without the help of jax)\n",
    "'''\n",
    "config['log_path'] = config['root_path']+'experiments/reports/'\n",
    "config['log'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "243ea694",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['poisoning_rate'] = 0.2\n",
    "# config['num_clean_examples'] = 200\n",
    "config['learning_rate'] = 0.01\n",
    "config['batch_size'] = 512\n",
    "config['num_epochs'] = 500\n",
    "config['backdoor_phrases'] = {0:['it was reported by bbc'], 1:['the taste was good but the service was not']}\n",
    "config['log']['attack'] = 'label-flip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e63fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_type = config['log']['attack']\n",
    "if attack_type == 'backdoor':\n",
    "    pass\n",
    "elif attack_type == 'influence':\n",
    "    config['attack_step_size'] = 0.1\n",
    "    config['num_iters'] = 1000\n",
    "elif attack_type == 'kkt':\n",
    "    config['log']['bad_loss_percentage'] = 30\n",
    "    config['log']['num_repeats'] = 5\n",
    "elif attack_type == 'label-flip':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59edca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['log']['model'] = 'SVM'\n",
    "config['log']['dataset'] = 'IMDB'\n",
    "config['log']['task'] = 'binary classification'\n",
    "config['log']['pytorch_seed'] = 50\n",
    "config['log']['numpy_seed'] = 50\n",
    "config['log']['method'] = 'modify'\n",
    "config['log']['space_dimension'] = 200\n",
    "config['log']['feature_extraction'] = 'wordcount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36802396",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['log_path'] += (str(config['series_ID'])+ '-' + config['log']['model'] + '-' + config['log']['attack'] + '-' +\n",
    "                       config['log']['dataset'] + '-' + config['log']['feature_extraction'] + '-' +\n",
    "                       str(int(config['poisoning_rate']*100)) + '-' +\n",
    "                       config['log']['method'] + '.json').lower().replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85cffa86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/user01/experiments/IMDB/SVM/103-label-flip-imdb-wordcount-20-modify.json'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['log_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a532f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WE NEED THIS TO IMPORT THE NECESSARY LIBRARIES ###\n",
    "import sys\n",
    "sys.path.append(config['root_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d98a7634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-20 01:29:03.368419: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-20 01:29:03.592685: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-20 01:29:04.306126: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: usr/local/cuda-11.8/lib64\n",
      "2022-11-20 01:29:04.306291: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: usr/local/cuda-11.8/lib64\n",
      "2022-11-20 01:29:04.306318: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import datascience, poisoning, report\n",
    "from datascience.data import CIFAR10, MNIST, IMDB, BOSTON\n",
    "from datascience.general import train_dev_test_split, join_np_arrays, describe_dataset, read_img, read_img_as_rgb, read_img_as_gray, resize_img, inverse_img, combine_single_channel_images, randomized_round\n",
    "from poisoning.process import attacker, defender, SVM_KKT_attacker, targeted_backdoor_attacker_img, targeted_backdoor_attacker_txt, LR_influence_attacker, SVM_influence_attacker, label_flip_attacker\n",
    "from poisoning.eval import attack_success_rate, benign_accuracy, test_accuracy\n",
    "from report.log import JSONLogger, TextLogger, tehran_datetime\n",
    "from temporary.functions import _reload\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchmetrics import HingeLoss\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd.functional import hessian, jacobian\n",
    "from torch.autograd import grad\n",
    "from torch.nn.utils import _stateless\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from persiantools.jdatetime import JalaliDate\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dd2bda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a5a19ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _reload(poisoning.process)\n",
    "# _reload(poisoning.eval)\n",
    "# _reload(datascience.data)\n",
    "# _reload(datascience.general)\n",
    "# _reload(report.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "211e1840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe9a8a26fd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(config['log']['pytorch_seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8684641a",
   "metadata": {},
   "source": [
    "# Loading a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a483ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20c077a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/user01/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74859baa1eb642f6a56bcb3b4a9dff63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = IMDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "203ef6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "attack_type = config['log']['attack']\n",
    "if attack_type == 'backdoor':\n",
    "    pass\n",
    "elif attack_type == 'influence':\n",
    "    dataset.random_sample_before_feature_extraction(1000,'train')\n",
    "    dataset.random_sample_before_feature_extraction(200,'test')\n",
    "    dataset.extract_features(config['log']['feature_extraction'],1,config['log']['space_dimension'],5)\n",
    "    config['data-train'] = describe_dataset(dataset.x_train, dataset.y_train, 'training dataset')\n",
    "    config['data-test'] = describe_dataset(dataset.x_test, dataset.y_test, 'testing dataset')\n",
    "    dataset.change_labels({0:-1})\n",
    "elif attack_type == 'kkt':\n",
    "    dataset.random_sample_before_feature_extraction(2000, 'train')\n",
    "    dataset.random_sample_before_feature_extraction(200, 'test')\n",
    "    dataset.extract_features(config['log']['feature_extraction'],1,config['log']['space_dimension'],5)\n",
    "    dataset.change_labels({0:-1})\n",
    "elif attack_type == 'label-flip':\n",
    "    dataset.extract_features(config['log']['feature_extraction'],1,config['log']['space_dimension'],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7d7a1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(dataset.y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e0b2a",
   "metadata": {},
   "source": [
    "# Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff3cf969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57 ms, sys: 7.87 ms, total: 64.9 ms\n",
      "Wall time: 63.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "attack_type = config['log']['attack']\n",
    "if attack_type == 'backdoor':\n",
    "    att = targeted_backdoor_attacker_txt(dataset.train, dataset.y_train, config['poisoning_rate'],\n",
    "                                         config['log']['method'], config['log']['numpy_seed'])\n",
    "    att.attack(config['backdoor_phrases'])\n",
    "    result = att.return_aggregated_result()\n",
    "    dataset.train = result['txt_train']\n",
    "    dataset.y_train = result['y_train']\n",
    "    dataset.extract_features(config['log']['feature_extraction'],1,config['log']['space_dimension'],5)\n",
    "    config['data-train'] = describe_dataset(dataset.x_train, dataset.y_train, 'training dataset')\n",
    "    \n",
    "elif attack_type == 'influence':\n",
    "    att = SVM_influence_attacker(dataset.x_train, dataset.y_train, dataset.x_test, dataset.y_test,\n",
    "                                 config['poisoning_rate'], config['attack_step_size'], config['log']['method'],\n",
    "                                 config['log']['numpy_seed'])\n",
    "    att.attack(num_iters=config['num_iters'])\n",
    "    att.x_poison = np.array([np.array([randomized_round(_xj) for _xj in _x]) for _x in att.x_poison])\n",
    "    result = att.return_aggregated_result()\n",
    "    result['y_train'] = np.array([1 if _y==1 else 0 for _y in result['y_train']])\n",
    "    dataset.revert_labels()\n",
    "    config['data-train'] = describe_dataset(result['x_train'], result['y_train'], 'training dataset')\n",
    "elif attack_type == 'kkt':\n",
    "    att = SVM_KKT_attacker(dataset.x_train, dataset.y_train, dataset.x_test, dataset.y_test,\n",
    "                       config['poisoning_rate'], config['log']['method'], config['log']['numpy_seed'])\n",
    "    att.find_decoy_params(config['log']['bad_loss_percentage'], config['log']['num_repeats'])\n",
    "    att.attack()\n",
    "    att.x_poison = np.array([np.array([randomized_round(_xj) for _xj in _x]) for _x in att.x_poison])\n",
    "    result = att.return_aggregated_result()\n",
    "    result['y_train'] = np.array([1 if _y==1 else 0 for _y in result['y_train']])\n",
    "    dataset.revert_labels()\n",
    "    config['data-train'] = describe_dataset(result['x_train'], result['y_train'], 'training dataset')\n",
    "elif attack_type == 'label-flip':\n",
    "    att = label_flip_attacker(dataset.x_train, dataset.y_train, {0:1,1:0}, config['poisoning_rate'],\n",
    "                          config['log']['method'], config['log']['numpy_seed'])\n",
    "    att.attack()\n",
    "    result = att.return_aggregated_result()\n",
    "    config['data-train'] = describe_dataset(result['x_train'], result['y_train'], 'training dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4670f7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['data-test'] = describe_dataset(dataset.x_test, dataset.y_test, 'testing dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b55c8770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(result['y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b556f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(dataset.y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbe9bc3",
   "metadata": {},
   "source": [
    "# Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7019407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVectorDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = np.array(labels).reshape(-1, 1)\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.Tensor(self.features[idx]).to(device), torch.Tensor(self.labels[idx]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4f2aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if attack_type == 'backdoor':\n",
    "    train_dataset = MyVectorDataset(dataset.x_train, dataset.y_train)\n",
    "elif attack_type in {'influence','kkt','label-flip'}:\n",
    "    train_dataset = MyVectorDataset(result['x_train'], result['y_train'])\n",
    "test_dataset = MyVectorDataset(dataset.x_test, dataset.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83c65a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "# clean_dataloader = DataLoader(clean_dataset, batch_size=config['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc938e5",
   "metadata": {},
   "source": [
    "# Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83e70154",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SVM, self).__init__() \n",
    "        self.linear = torch.nn.Linear(in_features=config['log']['space_dimension'], out_features=1, bias=True)\n",
    "    def forward(self, x):\n",
    "        output = self.linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64f6a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_label(out):\n",
    "    if out >= 0:\n",
    "      return 1\n",
    "    else:\n",
    "      return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d1f38ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, x_arr):\n",
    "  outs = list(model(torch.Tensor(x_arr)).squeeze().detach().numpy())\n",
    "  labels = [output_to_label(out) for out in outs]\n",
    "  return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6d9852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVM()\n",
    "model = model.to(device)\n",
    "loss_fn = HingeLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a49684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, epoch_num):\n",
    "    num_points = len(dataloader.dataset)\n",
    "    for batch, (features, labels) in enumerate(dataloader):        \n",
    "        # Compute prediction and loss\n",
    "        pred = model(features)\n",
    "        loss = loss_fn(pred, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # sets gradients of all model parameters to zero\n",
    "        loss.backward() # calculate the gradients again\n",
    "        optimizer.step() # w = w - learning_rate * grad(loss)_with_respect_to_w\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(features)\n",
    "            print(f\"\\r Epoch {epoch_num} - loss: {loss:>7f}  [{current:>5d}/{num_points:>5d}]\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05ee2555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn, epoch_num, name):\n",
    "    num_points = len(dataloader.dataset)\n",
    "    sum_test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (features, labels) in enumerate(dataloader):\n",
    "            pred = model(features)\n",
    "            sum_test_loss += loss_fn(pred, labels).item() # add the current loss to the sum of the losses\n",
    "            # convert the outputs of the model on the current batch to a numpy array\n",
    "            pred_lst = list(pred.cpu().numpy().squeeze())\n",
    "            pred_lst = [output_to_label(item) for item in pred_lst]\n",
    "            # convert the original labels corresponding to the current batch to a numpy array\n",
    "            output_lst = list(labels.cpu().numpy().squeeze()) \n",
    "            # determine the points for which the model is correctly predicting the label (add a 1 for each)\n",
    "            match_lst = [1 if p==o else 0 for (p, o) in zip(pred_lst, output_lst)] \n",
    "            # count how many points are labeled correctly in this batch and add the number to the overall count of the correct labeled points\n",
    "            correct += sum(match_lst) \n",
    "            \n",
    "    sum_test_loss /= num_points\n",
    "    correct /= num_points\n",
    "    config['log']['accuracy_'+name] = (100*correct)\n",
    "    config['log']['loss_'+name] = sum_test_loss\n",
    "    print(f\"\\r Epoch {epoch_num} - {name} Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {sum_test_loss:>8f}\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c791b772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 500 - loss: 0.747521  [    0/25000] CPU times: user 14min 32s, sys: 2.6 s, total: 14min 34s\n",
      "Wall time: 14min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch_num in range(1, config['num_epochs']+1):\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, epoch_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80574e5b",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f9242b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " Epoch 500 - Train Error: Accuracy: 67.2%, Avg loss: 0.001521 "
     ]
    }
   ],
   "source": [
    "test_loop(train_dataloader, model, loss_fn, config['num_epochs'], 'Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d40e21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " Epoch 500 - Test Error: Accuracy: 77.0%, Avg loss: 0.001157 "
     ]
    }
   ],
   "source": [
    "test_loop(test_dataloader, model, loss_fn, config['num_epochs'], 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0488af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if attack_type == 'backdoor':\n",
    "    y_train_pred = make_prediction(model.to('cpu'), dataset.x_train)\n",
    "elif attack_type in {'influence','kkt','label-flip'}:\n",
    "    y_train_pred = make_prediction(model.to('cpu'), result['x_train'])\n",
    "y_test_pred = make_prediction(model.to('cpu'), dataset.x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "138afd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['log']['benign_accuracy'] = benign_accuracy(result['y_train'], y_train_pred, result['is_poison'])\n",
    "config['log']['attack_success_rate'] = attack_success_rate(result['y_train'], y_train_pred, result['is_poison'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c145ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['datetime'] = tehran_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05ee27bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root_path': '/home/user01/',\n",
       " 'series_ID': 103,\n",
       " 'series_desc': '\\ntargeted backdoor attack labels changed - LR influence attack manually implemented (without the help of jax)\\n',\n",
       " 'log_path': '/home/user01/experiments/IMDB/SVM/103-label-flip-imdb-wordcount-20-modify.json',\n",
       " 'log': {'model': 'SVM',\n",
       "  'dataset': 'IMDB',\n",
       "  'task': 'binary classification',\n",
       "  'pytorch_seed': 50,\n",
       "  'numpy_seed': 50,\n",
       "  'attack': 'label-flip',\n",
       "  'method': 'modify',\n",
       "  'space_dimension': 200,\n",
       "  'bad_loss_percentage': 30,\n",
       "  'num_repeats': 5,\n",
       "  'feature_extraction': 'wordcount',\n",
       "  'accuracy_Train': 67.208,\n",
       "  'loss_Train': 0.001521251518726349,\n",
       "  'accuracy_Test': 77.03999999999999,\n",
       "  'loss_Test': 0.0011570309901237487,\n",
       "  'benign_accuracy': 0.7798,\n",
       "  'attack_success_rate': 0.2412},\n",
       " 'poisoning_rate': 0.2,\n",
       " 'learning_rate': 0.01,\n",
       " 'batch_size': 512,\n",
       " 'num_epochs': 500,\n",
       " 'attack_step_size': 0.1,\n",
       " 'num_iters': 1000,\n",
       " 'backdoor_phrases': {0: ['it was reported by bbc'],\n",
       "  1: ['the taste was good but the service was not']},\n",
       " 'data-train': {'name': 'training dataset',\n",
       "  'num_samples': 25000,\n",
       "  'num_features': (200,),\n",
       "  'class_count': {0: 12546, 1: 12454}},\n",
       " 'data-test': {'name': 'testing dataset',\n",
       "  'num_samples': 25000,\n",
       "  'num_features': (200,),\n",
       "  'class_count': {0: 12500, 1: 12500}},\n",
       " 'datetime': '1401-08-29 05:14'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "44bfda7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = JSONLogger(config['log_path'], config)\n",
    "# logger.log()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
