{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb9a5e2a",
   "metadata": {},
   "source": [
    "# Install Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b96fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install torchmetrics\n",
    "# !pip install pytz\n",
    "# !pip install persiantools\n",
    "# !pip install adversarial-robustness-toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62e286",
   "metadata": {},
   "source": [
    "# Google Drive Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8244ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "222ed25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a88fc6",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a51f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['root_path'] = '/home/user01/' # this is where the experiments folder exists\n",
    "config['series_ID'] = 106\n",
    "config['series_desc'] = '''\n",
    "first ideas to defend against attacks, having a trusted dataset\n",
    "'''\n",
    "config['log_path'] = config['root_path']+'experiments/reports/'\n",
    "config['log'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "243ea694",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['poisoning_rate'] = 0.4\n",
    "# config['num_clean_examples'] = 200\n",
    "config['learning_rate'] = 0.01\n",
    "config['batch_size'] = 32\n",
    "config['num_epochs'] = 2\n",
    "config['log']['attack'] = 'kkt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "609e608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_type = config['log']['attack']\n",
    "if attack_type == 'backdoor':\n",
    "    pass\n",
    "elif attack_type == 'influence':\n",
    "    config['num_iters'] = 100\n",
    "    config['attack_step_size'] = 0.5\n",
    "elif attack_type == 'kkt':\n",
    "    config['log']['bad_loss_percentage'] = 30\n",
    "    config['log']['num_repeats'] = 5\n",
    "elif attack_type == 'label-flip':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59edca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['log']['model'] = 'SVM'\n",
    "config['log']['dataset'] = 'MNIST (1-7)'\n",
    "config['log']['task'] = 'binary classification'\n",
    "config['log']['pytorch_seed'] = 50\n",
    "config['log']['numpy_seed'] = 50\n",
    "config['log']['num_backdoor_samples'] = 200\n",
    "config['log']['method'] = 'modify'\n",
    "config['log']['space_dimension'] = 225\n",
    "config['log']['img_width'] = 15\n",
    "config['log']['img_height'] = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36802396",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['log_path'] += (str(config['series_ID']) + '-' +config['log']['model'] + '-' + config['log']['attack'] +\n",
    "                       '-' + config['log']['dataset'] + '-' + str(int(config['poisoning_rate']*100)) +\n",
    "                       '-' + config['log']['method'] + '.json').lower().replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85cffa86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/user01/experiments/reports/106-svm-kkt-mnist(1-7)-40-modify.json'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['log_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a532f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WE NEED THIS TO IMPORT THE NECESSARY LIBRARIES ###\n",
    "import sys\n",
    "sys.path.append(config['root_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d98a7634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-07 04:14:49.804186: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-07 04:14:50.014000: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-07 04:14:50.922747: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: usr/local/cuda-11.8/lib64\n",
      "2022-12-07 04:14:50.922905: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: usr/local/cuda-11.8/lib64\n",
      "2022-12-07 04:14:50.922917: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import datascience, poisoning, report\n",
    "from datascience.data import CIFAR10, MNIST, IMDB, BOSTON\n",
    "from datascience.general import train_dev_test_split, join_np_arrays, describe_dataset, read_img, read_img_as_rgb, read_img_as_gray, resize_img, inverse_img, combine_single_channel_images, img_mixup, img_cutmix\n",
    "from poisoning.process import attacker, defender, SVM_KKT_attacker, targeted_backdoor_attacker_img, targeted_backdoor_attacker_txt, LR_influence_attacker, SVM_influence_attacker, label_flip_attacker, DPA_SVM\n",
    "from poisoning.eval import attack_success_rate, benign_accuracy, test_accuracy\n",
    "from report.log import JSONLogger, TextLogger, tehran_datetime\n",
    "from temporary.functions import _reload\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchmetrics import HingeLoss\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd.functional import hessian, jacobian\n",
    "from torch.autograd import grad\n",
    "from torch.nn.utils import _stateless\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from persiantools.jdatetime import JalaliDate\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dd2bda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/anaconda3/lib/python3.9/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /opt/conda/conda-bld/pytorch_1666643016022/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a5a19ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'report.log' from '/home/user01/report/log.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_reload(poisoning.process)\n",
    "_reload(poisoning.eval)\n",
    "_reload(datascience.data)\n",
    "_reload(datascience.general)\n",
    "_reload(report.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "211e1840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9249d9ff50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(config['log']['pytorch_seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a55168e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array([0.1,0.05,0.8,0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fa75d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 3, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(tmp, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "646df5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(tmp, axis=0)[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8684641a",
   "metadata": {},
   "source": [
    "# Loading a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20c077a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST()\n",
    "dataset.select_labels([1,7], phase='train')\n",
    "dataset.select_labels([1,7], phase='test')\n",
    "dataset.rescale()\n",
    "dataset.change_labels({7:-1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "203ef6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_type = config['log']['attack']\n",
    "if attack_type == 'backdoor':\n",
    "    if config['log']['img_width']!=28 or config['log']['img_height']!=28:\n",
    "        dataset.resize(config['log']['img_width'],config['log']['img_width'])\n",
    "elif attack_type == 'influence':\n",
    "    dataset.resize(config['log']['img_width'],config['log']['img_width'])\n",
    "    dataset.flatten()\n",
    "    dataset.random_sample(2000, 'train')\n",
    "    dataset.random_sample(200, 'test')\n",
    "elif attack_type == 'kkt':\n",
    "    dataset.resize(config['log']['img_width'],config['log']['img_width'])\n",
    "    dataset.flatten()\n",
    "    dataset.random_sample(2000, 'train')\n",
    "    dataset.random_sample(200, 'test')\n",
    "elif attack_type == 'label-flip':\n",
    "    dataset.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e0b2a",
   "metadata": {},
   "source": [
    "# Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c11a4a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mfn(x, y): # for 10x10\n",
    "#   tmp_x = x.copy()\n",
    "#   if y == 1:\n",
    "#     tmp_x[3,4] = 1.0\n",
    "#     tmp_x[4,4] = 1.0\n",
    "#     tmp_x[4,4] = 1.0\n",
    "#     tmp_x[5,4] = 1.0\n",
    "#     tmp_x[6,4] = 1.0\n",
    "#     tmp_x[7,4] = 1.0\n",
    "#     tmp_x[2,3] = 1.0\n",
    "#     tmp_x[2,2] = 1.0\n",
    "#     tmp_x[2,1] = 1.0\n",
    "#     tmp_x[3,1] = 1.0\n",
    "#     tmp_x[4,1] = 1.0\n",
    "#     tmp_x[5,1] = 1.0\n",
    "#     tmp_x[6,1] = 1.0\n",
    "#     tmp_x[7,1] = 1.0\n",
    "#     tmp_x[7,2] = 1.0\n",
    "#     tmp_x[7,3] = 1.0\n",
    "#   elif y == 0:\n",
    "#     tmp_x[2,4] = 1.0\n",
    "#     tmp_x[3,4] = 1.0\n",
    "#     tmp_x[4,4] = 1.0\n",
    "#     tmp_x[5,4] = 1.0\n",
    "#     tmp_x[6,4] = 1.0\n",
    "#     tmp_x[7,4] = 1.0\n",
    "#     tmp_x[3,3] = 1.0\n",
    "#   return tmp_x, 1-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c8865d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mfn(x, y): # for 15x15\n",
    "#   tmp_x = x.copy()\n",
    "#   if y == 1:\n",
    "#     tmp_x[9,11] = 1.0\n",
    "#     tmp_x[10,11] = 1.0\n",
    "#     tmp_x[11,11] = 1.0\n",
    "#     tmp_x[12,11] = 1.0\n",
    "#     tmp_x[13,11] = 1.0\n",
    "#     tmp_x[14,11] = 1.0\n",
    "#     tmp_x[9,10] = 1.0\n",
    "#     tmp_x[9,9] = 1.0\n",
    "#     tmp_x[9,8] = 1.0\n",
    "#     tmp_x[10,8] = 1.0\n",
    "#     tmp_x[11,8] = 1.0\n",
    "#     tmp_x[12,8] = 1.0\n",
    "#     tmp_x[13,8] = 1.0\n",
    "#     tmp_x[14,8] = 1.0\n",
    "#     tmp_x[14,9] = 1.0\n",
    "#     tmp_x[14,10] = 1.0\n",
    "#   elif y == 0:\n",
    "#     tmp_x[9,11] = 1.0\n",
    "#     tmp_x[10,11] = 1.0\n",
    "#     tmp_x[11,11] = 1.0\n",
    "#     tmp_x[12,11] = 1.0\n",
    "#     tmp_x[13,11] = 1.0\n",
    "#     tmp_x[14,11] = 1.0\n",
    "#     tmp_x[10,10] = 1.0\n",
    "#   return tmp_x, 1-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db64ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfn(x, y):\n",
    "  tmp_x = x.copy()\n",
    "  if y == 1:\n",
    "    tmp_x[20,21] = 1.0\n",
    "    tmp_x[21,21] = 1.0\n",
    "    tmp_x[22,21] = 1.0\n",
    "    tmp_x[23,21] = 1.0\n",
    "    tmp_x[24,21] = 1.0\n",
    "    tmp_x[25,21] = 1.0\n",
    "    tmp_x[20,20] = 1.0\n",
    "    tmp_x[20,19] = 1.0\n",
    "    tmp_x[20,18] = 1.0\n",
    "    tmp_x[21,18] = 1.0\n",
    "    tmp_x[22,18] = 1.0\n",
    "    tmp_x[23,18] = 1.0\n",
    "    tmp_x[24,18] = 1.0\n",
    "    tmp_x[25,18] = 1.0\n",
    "    tmp_x[25,19] = 1.0\n",
    "    tmp_x[25,20] = 1.0\n",
    "  elif y == -1:\n",
    "    tmp_x[20,21] = 1.0\n",
    "    tmp_x[21,21] = 1.0\n",
    "    tmp_x[22,21] = 1.0\n",
    "    tmp_x[23,21] = 1.0\n",
    "    tmp_x[24,21] = 1.0\n",
    "    tmp_x[25,21] = 1.0\n",
    "    tmp_x[20,20] = 1.0\n",
    "  return tmp_x, -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3cf969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py:569: RuntimeWarning: Method Nelder-Mead cannot handle constraints.\n",
      "  warn('Method %s cannot handle constraints.' % method,\n",
      "/home/user01/poisoning/process.py:194: OptimizeWarning: Initial guess is not within the specified bounds\n",
      "  optimization_result = scipy_minimize(objective_fn, x_0, method='Nelder-Mead',constraints=constraint,bounds=bounds)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "attack_type = config['log']['attack']\n",
    "if attack_type == 'backdoor':\n",
    "    att = targeted_backdoor_attacker_img(dataset.x_train, dataset.y_train, config['poisoning_rate'], mfn,\n",
    "                                     config['log']['method'], config['log']['numpy_seed'])\n",
    "    att.attack()\n",
    "    result = att.return_aggregated_result()\n",
    "    result['x_train'] = result['x_train'].reshape((-1, config['log']['img_width']*config['log']['img_height']))\n",
    "    dataset.flatten()\n",
    "elif attack_type == 'influence':\n",
    "    att = SVM_influence_attacker(dataset.x_train, dataset.y_train, dataset.x_test, dataset.y_test,\n",
    "                             config['poisoning_rate'], config['attack_step_size'], config['log']['method'],\n",
    "                                config['log']['pytorch_seed'], config['batch_size'], config['learning_rate'],\n",
    "                                config['num_epochs'])\n",
    "    att.attack(num_iters=config['num_iters'])\n",
    "    result = att.return_aggregated_result()\n",
    "#     result['y_train'] = np.array([1 if _y==1 else 0 for _y in result['y_train']])\n",
    "elif attack_type == 'kkt':\n",
    "    att = SVM_KKT_attacker(dataset.x_train, dataset.y_train, dataset.x_test, dataset.y_test,\n",
    "                       config['poisoning_rate'], config['log']['method'], config['log']['numpy_seed'])\n",
    "    att.find_decoy_params(config['log']['bad_loss_percentage'], config['log']['num_repeats'])\n",
    "    att.attack()\n",
    "    result = att.return_aggregated_result()\n",
    "elif attack_type == 'label-flip':\n",
    "    att = label_flip_attacker(dataset.x_train, dataset.y_train, {-1:1,1:-1}, config['poisoning_rate'],\n",
    "                          config['log']['method'], config['log']['numpy_seed'])\n",
    "    att.attack()\n",
    "    result = att.return_aggregated_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4670f7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['data-train'] = describe_dataset(result['x_train'], result['y_train'], 'training dataset')\n",
    "config['data-test'] = describe_dataset(dataset.x_test, dataset.y_test, 'testing dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55c8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(result['y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b556f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dataset.y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9b8c64",
   "metadata": {},
   "source": [
    "# Showcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee496b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_title = str(config['series_ID'])+'-'+config['log']['model']+'-'+config['log']['dataset']+'-'+str(config['poisoning_rate']*100)+'%-'+config['log']['attack']+'-'+config['log']['method']\n",
    "if config['poisoning_rate'] > 0.0:\n",
    "    rand_ints = np.random.randint(0, att.x_poison.shape[0], 16)\n",
    "    w = 20\n",
    "    h = 20\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    fig.suptitle(fig_title, fontsize=10)\n",
    "    columns = 4\n",
    "    rows = 4\n",
    "    for i in range(1, columns*rows +1):\n",
    "        img = att.x_poison[rand_ints[i-1]].reshape(config['log']['img_width'],config['log']['img_width'])\n",
    "        fig.add_subplot(rows, columns, i).title.set_text(att.y_poison[rand_ints[i-1]])\n",
    "        tmp = plt.imshow(img, cmap='gray')\n",
    "        tmp.axes.get_xaxis().set_visible(False)\n",
    "        tmp.axes.get_yaxis().set_visible(False)\n",
    "    plt.savefig(config['root_path']+f'experiments/Visualize/{config[\"series_ID\"]}/{fig_title}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbe9bc3",
   "metadata": {},
   "source": [
    "# Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVectorDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = np.array(labels).reshape(-1, 1)\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.Tensor(self.features[idx]).to(device), torch.Tensor(self.labels[idx]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = MyVectorDataset(dataset.x_train, dataset.y_train)\n",
    "train_dataset = MyVectorDataset(result['x_train'], result['y_train'])\n",
    "test_dataset = MyVectorDataset(dataset.x_test, dataset.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c65a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "# clean_dataloader = DataLoader(clean_dataset, batch_size=config['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc938e5",
   "metadata": {},
   "source": [
    "# Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e70154",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SVM, self).__init__() \n",
    "        self.linear = torch.nn.Linear(in_features=config['log']['space_dimension'], out_features=1, bias=True)\n",
    "    def forward(self, x):\n",
    "        output = self.linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f6a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_label(out):\n",
    "    if out >= 0:\n",
    "      return 1\n",
    "    else:\n",
    "      return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f38ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, x_arr):\n",
    "  outs = list(model(torch.Tensor(x_arr)).squeeze().detach().numpy())\n",
    "  labels = [output_to_label(out) for out in outs]\n",
    "  return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49694b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss_fn(preds, targets):\n",
    "    initial_value = 1-targets*preds\n",
    "    hinge_value = torch.mean(torch.clamp(initial_value, min=0))\n",
    "    return hinge_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVM()\n",
    "model = model.to(device)\n",
    "# loss_fn = HingeLoss()\n",
    "loss_fn = my_loss_fn\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate']) #, weight_decay=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a49684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, epoch_num):\n",
    "    num_points = len(dataloader.dataset)\n",
    "    for batch, (features, labels) in enumerate(dataloader):        \n",
    "        # Compute prediction and loss\n",
    "        pred = model(features)\n",
    "        loss = loss_fn(pred, labels) + 0.5*(torch.norm(model.linear.weight.squeeze())**2 + torch.norm(model.linear.bias.squeeze())**2)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # sets gradients of all model parameters to zero\n",
    "        loss.backward() # calculate the gradients again\n",
    "        optimizer.step() # w = w - learning_rate * grad(loss)_with_respect_to_w\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(features)\n",
    "            print(f\"\\r Epoch {epoch_num} - loss: {loss:>7f}  [{current:>5d}/{num_points:>5d}]\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ee2555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn, epoch_num, name):\n",
    "    num_points = len(dataloader.dataset)\n",
    "    sum_test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (features, labels) in enumerate(dataloader):\n",
    "            pred = model(features)\n",
    "            curr_loss = loss_fn(pred, labels) + 0.5*(torch.norm(model.linear.weight.squeeze())**2 + torch.norm(model.linear.bias.squeeze())**2)\n",
    "            sum_test_loss += curr_loss.item() # add the current loss to the sum of the losses\n",
    "            # convert the outputs of the model on the current batch to a numpy array\n",
    "            pred_lst = list(pred.cpu().numpy().squeeze())\n",
    "            pred_lst = [output_to_label(item) for item in pred_lst]\n",
    "            # convert the original labels corresponding to the current batch to a numpy array\n",
    "            output_lst = list(labels.cpu().numpy().squeeze()) \n",
    "            # determine the points for which the model is correctly predicting the label (add a 1 for each)\n",
    "            match_lst = [1 if p==o else 0 for (p, o) in zip(pred_lst, output_lst)] \n",
    "            # count how many points are labeled correctly in this batch and add the number to the overall count of the correct labeled points\n",
    "            correct += sum(match_lst) \n",
    "            \n",
    "    sum_test_loss /= num_points\n",
    "    correct /= num_points\n",
    "    config['log']['accuracy_'+name] = (100*correct)\n",
    "    config['log']['loss_'+name] = sum_test_loss\n",
    "    print(f\"\\r Epoch {epoch_num} - {name} Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {sum_test_loss:>8f}\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c791b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_num in range(1, config['num_epochs']+1):\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, epoch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a8d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dataset.y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80574e5b",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9242b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loop(train_dataloader, model, loss_fn, config['num_epochs'], 'Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d40e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loop(test_dataloader, model, loss_fn, config['num_epochs'], 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0488af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = make_prediction(model.to('cpu'), result['x_train'])\n",
    "y_test_pred = make_prediction(model.to('cpu'), dataset.x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c2ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_backdoor_dataset(x, y, transformer_fn, num_backdoor_samples):\n",
    "    num_samples = x.shape[0]\n",
    "    assert num_backdoor_samples <= num_samples, 'The number of backdoor samples should not exceed the number of samples in the original dataset'\n",
    "    chosen_indices = np.random.choice([i for i in range(num_samples)], num_backdoor_samples, replace=False)\n",
    "    x_chosen = x[chosen_indices].copy()\n",
    "    y_chosen = y[chosen_indices].copy()\n",
    "    x_backdoor = []\n",
    "    y_backdoor = []\n",
    "    for (_x, _y) in zip(x_chosen, y_chosen):\n",
    "        new_x, new_y = transformer_fn(_x.reshape(config['log']['img_width'],config['log']['img_height']), _y)\n",
    "        x_backdoor.append(new_x)\n",
    "        y_backdoor.append(new_y)\n",
    "    x_backdoor = np.array(x_backdoor).reshape(-1,config['log']['img_width']*config['log']['img_height'])\n",
    "    y_backdoor = np.array(y_backdoor)\n",
    "    return x_backdoor, y_backdoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4770646",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_type = config['log']['attack']\n",
    "if attack_type == 'backdoor':\n",
    "    x_backdoor, y_backdoor = make_backdoor_dataset(dataset.x_test, dataset.y_test, mfn, config['log']['num_backdoor_samples'])\n",
    "    y_backdoor_pred = make_prediction(model.to('cpu'), x_backdoor)\n",
    "    config['log']['attack_success_rate_test'] = test_accuracy(y_backdoor, y_backdoor_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138afd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['log']['benign_accuracy'] = benign_accuracy(result['y_train'], y_train_pred, result['is_poison'])\n",
    "config['log']['attack_success_rate'] = attack_success_rate(result['y_train'], y_train_pred, result['is_poison'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c145ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['datetime'] = tehran_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ee27bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bfda7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = JSONLogger(config['log_path'], config)\n",
    "# logger.log()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
